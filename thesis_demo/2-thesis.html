<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
            <meta name="Description" content="Basic Gray">
                <meta name="author" content="CoffeeCup Software, Inc.">
                    <meta name="Copyright" content="Copyright (c) 2010 CoffeeCup, all rights reserved.">
                        <title>Understand "GraphDNN" in 5min</title>
                        <link rel="stylesheet" href="stylesheets/default.css">
                            <script type="text/javascript" src="http://code.jquery.com/jquery-1.4.2.min.js"></script>
                            <script type="text/javascript" src="javascripts/behavior.js"></script>
                            <!--[if IE]>
                             <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
                             <![endif]-->
    </head>
    <body>
        <section id="intro3">
            <h2><center>Deep Neural Network Partitioning in Distributed Computing Systems</center></h2>
            
            <center>Jiyuan Shen</center>
            <h3>Goal</h3>
            <p>Partition DNN for working in distributed computing nodes.<br>
            <b>Specification:</b> Handle large scale deep neural networks to spread their work load including training and running across these distributed computing systems in pursuing further parallelism and higher performance.</p>
            
            <h3>My Work</h3>
            <p>
                &clubs; Proposed GraphDNN Framework to satisfy DNN partition in Distributed Computing Systems.<br>
                The framework can reduce costs to its 0.1*0.40715189=0.040715189.<br>
                &clubs; Implemented theoretical experiments.<br>
                &clubs; Implemented a real distributed system, and Experimented GraphDNN on real boards.
            </p>
            
            <h3>Method</h3>
            <p><b>Step 1:</b> Deep Compression: cut out synapses.<br>
             <b>Step 2:</b> Statically partition weight matrix: for fully-connected layers: find k-largest eigenvalues, and apply k-means to wigenvector; for convolutional layers: make simple copies to each distributed computing node. (notice the difference from the baseline partition.)<br>
            <!--<b>Example:</b> If this is what the camera sees (single object):<br>
            <center><img src="static_partition.png" alt="tracked video" style="width:500px;height:800px"></center>
            Then this is what its heat map looks like:<br>
            <center><img src="ui5m/HM/HM2D.gif" alt="heatmap 2d" style="width:150px;height:150px">
            <img src="ui5m/HM/HM3D.gif" alt="heatmap 3d" style="width:150px;height:150px"></center>
            Different types of activity form different heat map shapes, for example 2-object activities:<br>
            <center><img src="ui5m/HM/img1.png" alt="heatmaps" style="width:522px;height:300px"></center>-->
            <b>Step 3:</b> Dynamic Pruning: Keep the smallest weights.<br>
            <b>Step 4:</b> Greedy Cross-Weight Fixing: As possible as to FIX cross-weight as zeros, and CHANGE others to fine-tune the network.<br>
            <b>Example:</b> If we fix w_1 as zero, then it will only change w_2 as tuning.
            <center><img src="greedy.png" alt="example" style="width:500px;height:220px"></center>
            <!--<b>Example:</b> Original heat maps:<br>
            <center><img src="ui5m/HM/img2.png" alt="before alignment" style="width:478px;height:150px"></center>
            Aligned heat maps:<br>
            <center><img src="ui5m/HM/img3.png" alt="after alignment" style="width:478px;height:150px"></center>-->
            <b>Step 5:</b> Utilize Relu chances: predicti the output values.<br>
            </p>
            <h3>Example Diagram:</h3>
            <p>
            for a fully-connected network...<br>
            <center><img src="diagram.png" alt="example" style="width:800px;height:420px"></center>
            </p>
            
            <h3>References</h3>
            [1] Detailed PPTX Introduction. [<a href="thesis.pptx">pptx</a>]<br>
            [2] Jiyuan Shen, Deep Neural Network Partitioning in Distributed Computing Systems, Undergraduate Thesis, 2017 [<a href="https://shenjiyuan.github.io/files/thesis.pdf">pdf</a>]<br>
            [3] Spark and Caffe on TK1 boards, [<a href="Spark and Caffe on TK1 boards.docx">docx</a>]<br>
        </section>
    </body>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
         (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
         m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
         })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
         ga('create', 'UA-59284902-1', 'auto');
         ga('send', 'pageview');
        </script>
</html>
