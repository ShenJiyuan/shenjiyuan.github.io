<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Research and Experience -- Jiyuan Shen</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
					</header>

					<nav id="nav">
						<ul>
                            <li><a href="index.html" class="active"><b>Main</b></a></li>
                            <li><a href="education.html"><b>Education</b></a></li>
                            <li><a href="research.html"><b>Research and Experience</b></a></li>
                            <li><a href="project.html"><b>Project</b></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Content -->
						<section id="intro" class="main">
							<div class="spotlight">
								<div class="content">
                                    
                                    <header class="major">
                                        <h2><b>Experiences</b></h2>
                                    </header>
				    <img src="images/google_logo.png" width="220px" height="160px" style="float:left;margin-right:8px;"/>
                                    <br><b><a href="https://www.google.com/">Google</a>, Mountain View, U.S.</b>
                                    <br> Software Engineer Intern in <a href="https://adwords.google.com/home/#?modal_active=none">User Management in Google GPI & Ads</a>
                                    <br> Jun. 2018 - Sep. 2018
                                    <br><br><br><br>
									
                                    <img src="images/msra_logo.png" width="220px" height="120px" style="float:left;margin-right:8px;"/>
                                    <br><b><a href="http://www.msra.cn/">Microsoft Research Asia</a>, Beijing, China</b>
                                    <br> Research Intern in <a href="https://www.microsoft.com/en-us/research/group/systems-research-group-asia/">System Group</a>
                                    <br> Sept. 2016 - Jan. 2017
                                    <br><br><br>
                                    
                                    <img src="images/vcla_logo.png" width="220px" height="120px" style="float:left;margin-right:8px;"/>
                                    <br><b><a href="http://vcla.stat.ucla.edu/">Center for Vision, Cognition, Learning, and Autonomy</a>, Los Angeles</b>
                                    <br> Research Intern in Learning Group
                                    <br> Jul. 2016 - Sept. 2016
                                    <br><br><br>

                                </div>
							</div>
						</section>
                        
                        
                        <section id="projects" class="main">
                            <div class="spotlight">
                                <div class="content">
                                    <header class="major">
                                        <h2><b>Researches1: <br>Deep Neural Network Partitioning in Distributed Computing Systems, 2017</b></h2>
                                    </header>
                                    Partition DNN for working in distributed computing nodes. Handle large scale deep neural networks to spread their work load including training and running across these distributed computing systems in pursuing further parallelism and higher performance. I first utilize deep compression to compress the original neural network and cut off synapses as much. Then statically partition weight matrix: for fully-connected layers: find k-largest eigenvalues, and apply k-means to wigenvector; for convolutional layers: make simple copies to each distributed computing node. (notice the difference from the baseline partition.) Dynamic Pruning: Keep the smallest weights. Greedy Cross-Weight Fixing: As possible as to FIX cross-weight as zeros, and CHANGE others to fine-tune the network.
                                    
                                    <center><img src="images/thesis.png" width="700px" height="350px" style="float:left;margin-right:8px;"/></center><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
                                    
                                    <p><b>J. Shen</b>, "Deep Neural Network Partitioning in Distributed Computing Systems", 2017 [<a href="https://shenjiyuan.github.io/files/thesis.pdf">pdf</a>] [<a href="https://shenjiyuan.github.io/thesis_demo/2-thesis.html">demo</a>]<br>
                                    &bull; Proposed GraphDNN Framework to satisfy DNN partition in Distributed Computing Systems. The framework can reduce costs to its 0.1*0.40715189=0.040715189<br>
                                    &bull; Implemented theoretical experiments<br>
                                    &bull; Implemented a real distributed system, and Experimented GraphDNN on real boards
                                    </p>
                                </div>
                            </div>
                        </section>
                        
                        
                        <section id="projects" class="main">
                            <div class="spotlight">
                                <div class="content">
                                    <header class="major">
                                        <h2><b>Researches2: <br>Neural-network Retraining for Fault Tolerance, 2016</b></h2>
                                    </header>
                                    Leverage the inherent sparse weight-matrix and self-tuning capability of the neural network to avoid assigning the large value weight to the memristor with defects and high variance. We explore the self-healing of the neural-network to enlarge the solution space of finding the fault tolerable weight-memristor mappings. To train the neural-network, in this paper, we apply the conven- tional back-propagation method that utilizes the gradient descent technique. To show our retraining process with fixed weight, we use a simple two-layer fully connect neural-network as an example.
                                    
                                    <center><img src="images/map.png" width="700px" height="250px" style="float:left;margin-right:8px;"/></center><br><br><br><br><br><br><br><br><br><br>
                                    
                                    <p>L. Chen, J. Li, <b>J. Shen</b> and L. Jiang, "Learning Variations and Defects: a Neural-network Retraining Method for Fault Tolerance in the RRAM Crossbar" Design, Automation and Test in Europe (DATE), 2017 [<a href="https://shenjiyuan.github.io/files/memristor.pdf">pdf</a>] [<a href="https://shenjiyuan.github.io/memoristor_demo/1-memristor.html">demo</a>]<br>
                                    &bull; Designed redundancy combined with ”Kuhn-Munkres” mapping method applied to the model of variation on memristors given by Vortex(DAC15) where features include normal distribution, random presence and weight-error relations instead of hardware-style RRAM Crossbar computings<br>
                                    &bull; Implemented Redundancy-Mapping on the standard 784 × 10 Mnist Data Set (20−30% advanced)<br>
                                    &bull; Analyzed the relationship of topology and mapping and improvements of Redundancy-Mapping
                                    </p>
                                </div>
                            </div>
                        </section>


                        <section id="projects" class="main">
                            <div class="spotlight">
                                <div class="content">
                                    <header class="major">
                                        <h2><b>Researches3:<br> Leaf Three-dimensional Reconstruction with Multi-Photos, 2015</b></h2>
                                    </header>
                                    We first introduce feature detection sequence as KNN ratio test, symmetry test and RANSAC after basic SURF feature detection to get the most robust feature detector. A BruteForceMatcher obtains two nearest neighbors through comparing the descriptors that removes bad matches then further checked by DMatches to delete all non-symmetrical matches. Finally RANSAC returns the fundamental matrix. Experiments show 30 matches for threshold would be promising. We also introduce Lookup Table that stores 2D coordinates of current frame and its triangled 3D coordinates, then by keeping track of 2D-3D pair during each iteration of processing new frames we can minimizes projection errors. We quantify the effect of generated contour by fine points and observation. Experiments on real-world datasets show effectiveness of our algorithm and improvements.
                                    
                                    <center><img src="images/leaf.png" width="700px" height="250px" style="float:left;margin-right:8px;"/></center><br><br><br><br><br><br><br><br><br><br>
                                    
                                    <p> <b>J. Shen</b>, X. Yang and Z. Fan, "3D Reconstruction of Plant Leaves from Rough Multi-Photos" [<a href="https://shenjiyuan.github.io/files/leave.pdf">pdf</a>] [<a href="https://shenjiyuan.github.io/3-3D_demo/3-leaves.html">demo</a>]<br>
                                    &bull; Designed the Filtering Feature Sequence as KNN ratio test, symmetry test and RANSAC after SURF<br>
                                    &bull; Designed Surface-Stereo Lookup Table that stores current frame plane coordinates with its accordingly triangle three-dimensional coordinates for tracking processing iterations <br>
                                    &bull; Implemented the three-dimensional reconstruction on random reddish-green Epipremnum aureum
                                    </p>
                                </div>
                            </div>
                        </section>

                        </div>

				<!-- Footer -->
					<footer id="footer">
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
